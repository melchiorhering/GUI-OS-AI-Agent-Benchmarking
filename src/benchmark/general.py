import re
import shutil
from pathlib import Path
from typing import Dict, List

from smolagents import LogLevel

from agent.sandbox_agent import SandboxCodeAgent

from .configuration import download_file_from_vm, upload_script_and_execute
from .task import TaskInput


def compare_text_file(
    agent: SandboxCodeAgent,
    task: TaskInput,
    local_expected_file: str,
    vm_result_file: str,
    **options,
) -> float:
    task.result_dir.mkdir(parents=True, exist_ok=True)
    local_vm_result_path = task.result_dir / Path(vm_result_file).name
    source_expected_file_path = task.task_dir / Path(local_expected_file)
    local_expected_reference_path = task.result_dir / Path(local_expected_file).name

    agent.logger.log(f"ðŸ“„ Comparing text file for '{task.uid}'.", level=LogLevel.INFO)
    agent.logger.log(f"  VM: '{vm_result_file}' | Expected: '{local_expected_file}'", level=LogLevel.DEBUG)

    try:
        download_file_from_vm(agent, local_path=local_vm_result_path, remote_path=vm_result_file)

        if not source_expected_file_path.is_file():
            agent.logger.log(f"âŒ Expected file not found: '{source_expected_file_path}'", level=LogLevel.ERROR)
            return 0.0
        shutil.copy2(source_expected_file_path, local_expected_reference_path)

        with open(local_vm_result_path, "r", encoding="utf-8") as f1:
            actual_text = f1.read()
        with open(local_expected_reference_path, "r", encoding="utf-8") as f2:
            expected_text = f2.read()

        ignore_blanks = options.get("ignore_blanks", False)
        if ignore_blanks:
            actual_text = re.sub(r"[\t\n]", " ", actual_text).strip()
            actual_text = re.sub(r"\s+", " ", actual_text)
            expected_text = re.sub(r"[\t\n]", " ", expected_text).strip()
            expected_text = re.sub(r"\s+", " ", expected_text)

        ignore_case = options.get("ignore_case", False)
        if ignore_case:
            actual_text = actual_text.lower()
            expected_text = expected_text.lower()

        if actual_text == expected_text:
            agent.logger.log(f"âœ… Text file comparison PASSED for '{task.uid}'. Files match.", level=LogLevel.INFO)
            return 1.0
        else:
            agent.logger.log(
                f"âŒ Text file comparison FAILED for '{task.uid}'. Files do not match.", level=LogLevel.INFO
            )
            return 0.0

    except Exception as e:
        agent.logger.log(f"âŒ Error during text file comparison for '{task.uid}': {e}", level=LogLevel.ERROR)
        return 0.0


def check_include_exclude(result: str, rules: Dict[str, List[str]]) -> float:
    if result is None:
        return 0.0

    print(result, rules)
    include = rules.get("include", [])
    exclude = rules.get("exclude", [])
    if all(r in result for r in include) and all(r not in result for r in exclude):
        return 1.0
    else:
        return 0.0


def compare_script_logs(
    agent: SandboxCodeAgent,
    task: TaskInput,
    eval_script: str,  # Path to the local evaluation script (this needs to be uploaded to the VM and run) for example eval.sh
    vm_eval_logs: str,  # Path to the log file generated by the script on VM # for example eval.log
    **options,
) -> float:
    """
    Executes an evaluation script on the VM, retrieves its log,
    and parses the result to determine if the Jupyter kernel setup was successful.
    """
    task.result_dir.mkdir(parents=True, exist_ok=True)

    # Define paths for local and remote files & upload and run the script in VM
    remote_eval_script_path = f"/home/user/{Path(eval_script).name}"
    upload_script_and_execute(agent, task, local_path=eval_script, remote_path=remote_eval_script_path)

    # Retrieve the log file from the VM
    local_eval_logs_path = task.result_dir / Path(vm_eval_logs).name
    download_file_from_vm(agent, local_eval_logs_path, remote_path=vm_eval_logs)

    # Read the downloaded log file
    log_content = ""
    try:
        with open(local_eval_logs_path, "r") as f:
            log_content = f.read()
    except FileNotFoundError:
        agent.logger.log_error(
            f"Error: Log file not found at {local_eval_logs_path}",
        )
        return 0.0
    except Exception as e:
        agent.logger.log_error(
            f"Error reading log file {local_eval_logs_path}: {e}",
        )
        return 0.0

    # Extract rules from options
    rules = options.get("rules", {})
    if not rules:
        agent.logger.log_error(
            "Warning: 'rules' not found in options. Returning 0.0.",
        )
        return 0.0

    # Use the check_include_exclude function to evaluate the log content
    score = check_include_exclude(log_content, rules)
    agent.logger.log(f"Evaluation score: {score}", level=LogLevel.INFO)

    return score
